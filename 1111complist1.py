# -*- coding: utf-8 -*-
"""1111CompList.ipynb

Automatically generated by Colaboratory.


# 載入套件
"""

import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import datetime



headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36'}

"""# 收集公司清單

"""

urls = []
for i in range(200):
    page = 'https://www.1111.com.tw/job-bank/job-index.asp?si=2&fs=1&ps=100&page={}'.format(str(i+1))
    resp = requests.get(page, headers=headers)
    nurls = list(set(re.findall('www.1111.com.tw/corp/[0-9]{1,}/', resp.text)))
    print(nurls)# 抓取的網抓取的網址
    # 如果加載新分頁沒有抓到新的資料，就停止迴圈
    if len(urls) == len(list(set(urls + nurls))):
        break
    urls = list(set(urls + nurls))
    print(len(urls))

urls[:10]

"""# 爬取個別公司資料
- 網頁中有許多資訊，而我們抓的資料可以分成以下幾個區塊
- 基本資料
- 公司簡介
- 產品服務
- 福利制度
"""

url = 'www.1111.com.tw/corp/9721259/'
resp = requests.get('https://' + url)
print(resp.url)
soup = BeautifulSoup(resp.text,'lxml')

"""## 基本資料區"""

def BasicInfo(soup):
    adress, industry, capital, tax_id, emp_num = [''] * 5
    for i in soup.select('section > ul > li'):
        if '公司位置' in i.text:
            adress = i.find('div',{'class':'dd'}).text
        if '產業類別' in  i.text:
            industry = i.find('div',{'class':'dd'}).text
      
           # industry_exp = i.find('div',{'class':'left text--lightgrey'}).text
        if '資本額' in  i.text:
            capital = i.find('div',{'class':'dd'}).text
        if '公司統編' in i.text:
            tax_id = i.find('div',{'class':'dd'}).text
        if '公司規模' in  i.text:
            emp_num = i.find('div',{'class':'dd'}).text


            
    return adress, industry, capital, tax_id, emp_num

"""## 公司簡介"""

# 公司簡介
' '.join(i.text for i in soup.select('div[class="companyProfileText"]'))

"""## 產品服務"""

# 產品服務
' '.join(i.text for i in soup.select('div[class="text-list"]'))#.select_one('li').text

"""## 員工福利"""

def Emp_Welfare(soup):
    legal, welfare = [''] * 2
    for i in soup.select('div[id="c3"] > div[class="wrap"] > div > p'):
        if '法定項目' in i.text:
            legal = i.text
        if '福利制度' in i.text:
            welfare = i.text
    return legal, welfare

legal, welfare = Emp_Welfare(soup)
print(legal)
print(welfare)

"""## 職缺數量
- 因為職缺數量是在另外一個網址送 requests 查詢回來的，所以在這邊另外寫一個爬蟲抓職缺數量的資料
"""

#def JobCounts(NO):
 #   url = 'https://www.1111.com.tw/job-bank/company-description-joblist.asp?nNo=' + NO
  #  resp = requests.get(url)
   # soup = BeautifulSoup(resp.text)
    #count = soup.find('div',id = 'recruitment-company-button')
    #count1 = count.find('span').text
    #return count1

#JobCounts('9721259')

"""## 組合函數
把以上函數組合起來應用，讓我們可以輸入網址就輕鬆整理成表格
"""

def url2df(url):
    print('https://' + url)
    NO = url.split('/',-1)[-2]
    resp = requests.get('https://' + url)
    soup = BeautifulSoup(resp.text)
    # 公司名稱
    comp_name = soup.select('ul > li')[0].text    
    # 基本資訊
    adress, industry, capital, tax_id, emp_num = BasicInfo(soup)
    # 公司簡介
    comp_intro = ' '.join(i.text for i in soup.select('div[class="companyProfileText"]'))
    # 產品服務
    try:
        comp_ser = ' '.join(i.text for i in soup.select('div[class="text-list"]'))
    except:
        comp_ser = ''
    # 員工福利
    legal, welfare = Emp_Welfare(soup)    
    # 職缺數量
    #jobcounts = JobCounts(NO)
    ndf = pd.DataFrame([{'編號':NO,
                         '公司名稱': comp_name,
                         '關鍵詞': soup.find('meta',{'name':'keywords'})['content'],
                         '公司簡介':comp_intro,
                         '產品服務':comp_ser,
                         '法定福利':legal,
                         '公司福利':welfare,
                         #'職缺數量':jobcounts,
                         '聯絡地址':adress,
                         '行業別':industry,
                         #'行業說明':industry_exp,
                         '資本額':capital,
                         #'成立時間':established,
                         '統一編號':tax_id,
                         '員工人數':emp_num,
                         #'公司電話':comp_pho,
                         #'公司傳真':comp_fax,
                         #'網站位址':website,
                         #'相關連結':relate_url,
                         '連結網址':resp.url,
                         '更新時間':datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}],
                       columns=['編號','公司名稱','關鍵詞','公司簡介','產品服務','法定福利','公司福利','職缺數量','聯絡地址', '行業別','行業說明',
                                '資本額','成立時間','統一編號', '員工人數','公司電話','公司傳真','網站位址','相關連結','連結網址','更新時間'])
    return ndf

# url = 'www.1111.com.tw/corp/71724069/'
# url = 'www.1111.com.tw/corp/9721259/'
url = 'www.1111.com.tw/corp/69622670/'
url2df(url)

"""## 執行爬蟲
- 將前面收集到的公司清單逐一透過組合好的函數內爬蟲
"""

df = []
fail = []
for i, url in enumerate(urls):
    if i % 100 == 0:
        print(i)
    try:
        ndf = url2df(url)
        df.append(ndf)
    except:
        fail.append(url)

df = pd.concat(df, ignore_index=True)
df

fail

"""# 保存資料
- 先跟 Google Drive 空間串接，再將資料存在 Google Drive 上
- 資料放在 Google Drive 上的 Colab Notebooks 資料夾中
"""

import os
from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/My Drive/Colab Notebooks/'
df.to_excel(path + '1111CompList.xlsx')
